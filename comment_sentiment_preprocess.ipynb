{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Comment Sentiment Preprocessing\n",
    "## Quick Overview\n",
    "This notebook preprocesses a dataset of Reddit comments for sentiment analysis. The focus is on cleaning and preparing the data for sentiment scoring.\n",
    "\n",
    "## Steps\n",
    "- Data Loading: Efficiently load Reddit comments in chunks.\n",
    "- Text Cleaning: Remove irrelevant text components.\n",
    "- Sentiment Analysis: Perform nltk's SentimentIntensityAnalyzer to all comments.\n",
    "- Batch Processing: Handle data in manageable batches for system efficiency.\n",
    "- Calculated the weighted sentiment score, which is calculate by multiply the sentiment score by the number of upvotes. if the number of upvotes is 0, use score value 0.01 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_sentiment_scores(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores['neg'], scores['neu'], scores['pos'], scores['compound']\n",
    "\n",
    "def calculate_weighted_sentiment(score, sentiment_score):\n",
    "    return score * sentiment_score if score != 0 else 0.01 * sentiment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 processed\n",
      "Chunk 2 processed\n",
      "Chunk 3 processed\n",
      "Chunk 4 processed\n",
      "Chunk 5 processed\n",
      "Chunk 6 processed\n",
      "Chunk 7 processed\n",
      "Chunk 8 processed\n",
      "Chunk 9 processed\n",
      "Chunk 10 processed\n",
      "Chunk 11 processed\n",
      "Chunk 12 processed\n",
      "Chunk 13 processed\n",
      "Chunk 14 processed\n",
      "Chunk 15 processed\n",
      "Chunk 16 processed\n",
      "Chunk 17 processed\n",
      "Chunk 18 processed\n",
      "Chunk 19 processed\n",
      "Chunk 20 processed\n",
      "Chunk 21 processed\n",
      "Chunk 22 processed\n",
      "Chunk 23 processed\n",
      "Chunk 24 processed\n",
      "Chunk 25 processed\n",
      "Chunk 26 processed\n",
      "Chunk 27 processed\n",
      "Chunk 28 processed\n",
      "Chunk 29 processed\n",
      "Chunk 30 processed\n",
      "Chunk 31 processed\n",
      "Chunk 32 processed\n",
      "Chunk 33 processed\n",
      "Chunk 34 processed\n",
      "Chunk 35 processed\n",
      "Chunk 36 processed\n",
      "Chunk 37 processed\n",
      "Chunk 38 processed\n",
      "Chunk 39 processed\n",
      "Chunk 40 processed\n",
      "Chunk 41 processed\n",
      "Chunk 42 processed\n",
      "Chunk 43 processed\n",
      "Chunk 44 processed\n",
      "Chunk 45 processed\n",
      "Chunk 46 processed\n",
      "Chunk 47 processed\n",
      "Chunk 48 processed\n",
      "Chunk 49 processed\n",
      "Chunk 50 processed\n",
      "Chunk 51 processed\n",
      "Chunk 52 processed\n",
      "Chunk 53 processed\n",
      "Chunk 54 processed\n",
      "Chunk 55 processed\n",
      "Chunk 56 processed\n",
      "Chunk 57 processed\n",
      "Chunk 58 processed\n",
      "Chunk 59 processed\n",
      "Chunk 60 processed\n",
      "Chunk 61 processed\n",
      "Chunk 62 processed\n",
      "Chunk 63 processed\n",
      "Chunk 64 processed\n",
      "Chunk 65 processed\n",
      "Chunk 66 processed\n",
      "Chunk 67 processed\n",
      "Chunk 68 processed\n",
      "Chunk 69 processed\n",
      "Chunk 70 processed\n",
      "Chunk 71 processed\n",
      "Chunk 72 processed\n",
      "Chunk 73 processed\n",
      "Chunk 74 processed\n",
      "Chunk 75 processed\n",
      "Chunk 76 processed\n",
      "Chunk 77 processed\n",
      "Chunk 78 processed\n",
      "Chunk 79 processed\n",
      "Chunk 80 processed\n",
      "Chunk 81 processed\n",
      "Chunk 82 processed\n",
      "Chunk 83 processed\n",
      "Chunk 84 processed\n",
      "Chunk 85 processed\n",
      "Chunk 86 processed\n",
      "Chunk 87 processed\n",
      "Chunk 88 processed\n",
      "Chunk 89 processed\n",
      "Chunk 90 processed\n",
      "Chunk 91 processed\n",
      "Chunk 92 processed\n",
      "Chunk 93 processed\n",
      "Chunk 94 processed\n",
      "Chunk 95 processed\n",
      "Chunk 96 processed\n",
      "Chunk 97 processed\n",
      "Chunk 98 processed\n",
      "Chunk 99 processed\n",
      "Chunk 100 processed\n"
     ]
    }
   ],
   "source": [
    "chunk_count = 0\n",
    "\n",
    "chunk_size = 100000  \n",
    "processed_data = []\n",
    "\n",
    "for chunk in pd.read_csv('text_comments.csv', chunksize=chunk_size, on_bad_lines='skip',lineterminator='\\n',nrows=10000000):\n",
    "    # Drop rows where 'body' is null\n",
    "    chunk = chunk.dropna(subset=['body'])\n",
    "\n",
    "    # Convert 'created_utc' to standard datetime\n",
    "    chunk['created_utc'] = pd.to_datetime(chunk['created_utc'], unit='s')\n",
    "\n",
    "    # Calculate sentiment scores\n",
    "    chunk[['negative', 'neutral', 'positive', 'compound']] = chunk['body'].apply(lambda x: calculate_sentiment_scores(x)).apply(pd.Series)\n",
    "    \n",
    "    # Calculate weighted sentiment scores\n",
    "    for sentiment in ['negative', 'neutral', 'positive', 'compound']:\n",
    "        chunk[f'weighted_{sentiment}'] = chunk.apply(lambda row: calculate_weighted_sentiment(row['score'], row[sentiment]), axis=1)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    chunk = chunk[['id', 'score', 'link_id', 'subreddit', 'created_utc', 'negative', 'neutral', 'positive', 'compound', 'weighted_negative', 'weighted_neutral', 'weighted_positive', 'weighted_compound']]\n",
    "    \n",
    "    processed_data.append(chunk)\n",
    "    chunk_count += 1\n",
    "    print(f\"Chunk {chunk_count} processed\")\n",
    "\n",
    "# Concatenate all processed chunks\n",
    "final_df = pd.concat(processed_data)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('processed_comments1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 101 processed\n",
      "Chunk 102 processed\n",
      "Chunk 103 processed\n",
      "Chunk 104 processed\n",
      "Chunk 105 processed\n",
      "Chunk 106 processed\n",
      "Chunk 107 processed\n",
      "Chunk 108 processed\n",
      "Chunk 109 processed\n",
      "Chunk 110 processed\n",
      "Chunk 111 processed\n",
      "Chunk 112 processed\n",
      "Chunk 113 processed\n",
      "Chunk 114 processed\n",
      "Chunk 115 processed\n",
      "Chunk 116 processed\n",
      "Chunk 117 processed\n",
      "Chunk 118 processed\n",
      "Chunk 119 processed\n",
      "Chunk 120 processed\n",
      "Chunk 121 processed\n",
      "Chunk 122 processed\n",
      "Chunk 123 processed\n",
      "Chunk 124 processed\n",
      "Chunk 125 processed\n",
      "Chunk 126 processed\n",
      "Chunk 127 processed\n",
      "Chunk 128 processed\n",
      "Chunk 129 processed\n",
      "Chunk 130 processed\n",
      "Chunk 131 processed\n",
      "Chunk 132 processed\n",
      "Chunk 133 processed\n",
      "Chunk 134 processed\n",
      "Chunk 135 processed\n",
      "Chunk 136 processed\n",
      "Chunk 137 processed\n",
      "Chunk 138 processed\n",
      "Chunk 139 processed\n",
      "Chunk 140 processed\n",
      "Chunk 141 processed\n",
      "Chunk 142 processed\n",
      "Chunk 143 processed\n",
      "Chunk 144 processed\n",
      "Chunk 145 processed\n",
      "Chunk 146 processed\n",
      "Chunk 147 processed\n",
      "Chunk 148 processed\n",
      "Chunk 149 processed\n",
      "Chunk 150 processed\n",
      "Chunk 151 processed\n",
      "Chunk 152 processed\n",
      "Chunk 153 processed\n",
      "Chunk 154 processed\n",
      "Chunk 155 processed\n",
      "Chunk 156 processed\n",
      "Chunk 157 processed\n",
      "Chunk 158 processed\n",
      "Chunk 159 processed\n",
      "Chunk 160 processed\n",
      "Chunk 161 processed\n",
      "Chunk 162 processed\n",
      "Chunk 163 processed\n",
      "Chunk 164 processed\n",
      "Chunk 165 processed\n",
      "Chunk 166 processed\n",
      "Chunk 167 processed\n",
      "Chunk 168 processed\n",
      "Chunk 169 processed\n",
      "Chunk 170 processed\n",
      "Chunk 171 processed\n",
      "Chunk 172 processed\n",
      "Chunk 173 processed\n",
      "Chunk 174 processed\n",
      "Chunk 175 processed\n",
      "Chunk 176 processed\n",
      "Chunk 177 processed\n",
      "Chunk 178 processed\n",
      "Chunk 179 processed\n",
      "Chunk 180 processed\n",
      "Chunk 181 processed\n",
      "Chunk 182 processed\n",
      "Chunk 183 processed\n",
      "Chunk 184 processed\n",
      "Chunk 185 processed\n",
      "Chunk 186 processed\n",
      "Chunk 187 processed\n",
      "Chunk 188 processed\n",
      "Chunk 189 processed\n",
      "Chunk 190 processed\n",
      "Chunk 191 processed\n",
      "Chunk 192 processed\n",
      "Chunk 193 processed\n",
      "Chunk 194 processed\n",
      "Chunk 195 processed\n",
      "Chunk 196 processed\n",
      "Chunk 197 processed\n",
      "Chunk 198 processed\n",
      "Chunk 199 processed\n",
      "Chunk 200 processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of rows to skip (10,000,000 already processed) and number of rows to read\n",
    "rows_to_skip = 10000000\n",
    "rows_to_read = 10000000\n",
    "\n",
    "# Read the CSV file in chunks, skipping the first 10,000,000 rows\n",
    "for chunk in pd.read_csv('text_comments.csv', chunksize=chunk_size, skiprows=range(1, rows_to_skip), nrows=rows_to_read, on_bad_lines='skip', lineterminator='\\n'):\n",
    "    # Drop rows where 'body' is null\n",
    "    chunk = chunk.dropna(subset=['body'])\n",
    "\n",
    "    # Convert 'created_utc' to standard datetime\n",
    "    chunk['created_utc'] = pd.to_datetime(chunk['created_utc'], unit='s')\n",
    "\n",
    "    # Calculate sentiment scores\n",
    "    chunk[['negative', 'neutral', 'positive', 'compound']] = chunk['body'].apply(lambda x: calculate_sentiment_scores(x)).apply(pd.Series)\n",
    "    \n",
    "    # Calculate weighted sentiment scores\n",
    "    for sentiment in ['negative', 'neutral', 'positive', 'compound']:\n",
    "        chunk[f'weighted_{sentiment}'] = chunk.apply(lambda row: calculate_weighted_sentiment(row['score'], row[sentiment]), axis=1)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    chunk = chunk[['id', 'score', 'link_id', 'subreddit', 'created_utc', 'negative', 'neutral', 'positive', 'compound', 'weighted_negative', 'weighted_neutral', 'weighted_positive', 'weighted_compound']]\n",
    "    \n",
    "    processed_data.append(chunk)\n",
    "    chunk_count += 1\n",
    "    print(f\"Chunk {chunk_count} processed\")\n",
    "\n",
    "# Concatenate all processed chunks\n",
    "final_df = pd.concat(processed_data)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('processed_comments2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 201 processed\n",
      "Chunk 202 processed\n",
      "Chunk 203 processed\n",
      "Chunk 204 processed\n",
      "Chunk 205 processed\n",
      "Chunk 206 processed\n",
      "Chunk 207 processed\n",
      "Chunk 208 processed\n",
      "Chunk 209 processed\n",
      "Chunk 210 processed\n",
      "Chunk 211 processed\n",
      "Chunk 212 processed\n",
      "Chunk 213 processed\n",
      "Chunk 214 processed\n",
      "Chunk 215 processed\n",
      "Chunk 216 processed\n",
      "Chunk 217 processed\n",
      "Chunk 218 processed\n",
      "Chunk 219 processed\n",
      "Chunk 220 processed\n",
      "Chunk 221 processed\n",
      "Chunk 222 processed\n",
      "Chunk 223 processed\n",
      "Chunk 224 processed\n",
      "Chunk 225 processed\n",
      "Chunk 226 processed\n",
      "Chunk 227 processed\n",
      "Chunk 228 processed\n",
      "Chunk 229 processed\n",
      "Chunk 230 processed\n",
      "Chunk 231 processed\n",
      "Chunk 232 processed\n",
      "Chunk 233 processed\n",
      "Chunk 234 processed\n",
      "Chunk 235 processed\n",
      "Chunk 236 processed\n",
      "Chunk 237 processed\n",
      "Chunk 238 processed\n",
      "Chunk 239 processed\n",
      "Chunk 240 processed\n",
      "Chunk 241 processed\n",
      "Chunk 242 processed\n",
      "Chunk 243 processed\n",
      "Chunk 244 processed\n",
      "Chunk 245 processed\n",
      "Chunk 246 processed\n",
      "Chunk 247 processed\n",
      "Chunk 248 processed\n",
      "Chunk 249 processed\n",
      "Chunk 250 processed\n",
      "Chunk 251 processed\n",
      "Chunk 252 processed\n",
      "Chunk 253 processed\n",
      "Chunk 254 processed\n",
      "Chunk 255 processed\n",
      "Chunk 256 processed\n",
      "Chunk 257 processed\n",
      "Chunk 258 processed\n",
      "Chunk 259 processed\n",
      "Chunk 260 processed\n",
      "Chunk 261 processed\n",
      "Chunk 262 processed\n",
      "Chunk 263 processed\n",
      "Chunk 264 processed\n",
      "Chunk 265 processed\n",
      "Chunk 266 processed\n",
      "Chunk 267 processed\n",
      "Chunk 268 processed\n",
      "Chunk 269 processed\n",
      "Chunk 270 processed\n",
      "Chunk 271 processed\n",
      "Chunk 272 processed\n",
      "Chunk 273 processed\n",
      "Chunk 274 processed\n",
      "Chunk 275 processed\n",
      "Chunk 276 processed\n",
      "Chunk 277 processed\n",
      "Chunk 278 processed\n",
      "Chunk 279 processed\n",
      "Chunk 280 processed\n",
      "Chunk 281 processed\n",
      "Chunk 282 processed\n",
      "Chunk 283 processed\n",
      "Chunk 284 processed\n",
      "Chunk 285 processed\n",
      "Chunk 286 processed\n",
      "Chunk 287 processed\n",
      "Chunk 288 processed\n",
      "Chunk 289 processed\n",
      "Chunk 290 processed\n",
      "Chunk 291 processed\n",
      "Chunk 292 processed\n",
      "Chunk 293 processed\n",
      "Chunk 294 processed\n",
      "Chunk 295 processed\n",
      "Chunk 296 processed\n",
      "Chunk 297 processed\n",
      "Chunk 298 processed\n",
      "Chunk 299 processed\n",
      "Chunk 300 processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of rows to skip (10,000,000 already processed) and number of rows to read\n",
    "rows_to_skip = 20000000\n",
    "rows_to_read = 10000000\n",
    "\n",
    "# Read the CSV file in chunks, skipping the first 20,000,000 rows\n",
    "for chunk in pd.read_csv('text_comments.csv', chunksize=chunk_size, skiprows=range(1, rows_to_skip), nrows=rows_to_read, on_bad_lines='skip', lineterminator='\\n'):\n",
    "    # Drop rows where 'body' is null\n",
    "    chunk = chunk.dropna(subset=['body'])\n",
    "\n",
    "    # Convert 'created_utc' to standard datetime\n",
    "    chunk['created_utc'] = pd.to_datetime(chunk['created_utc'], unit='s')\n",
    "\n",
    "    # Calculate sentiment scores\n",
    "    chunk[['negative', 'neutral', 'positive', 'compound']] = chunk['body'].apply(lambda x: calculate_sentiment_scores(x)).apply(pd.Series)\n",
    "    \n",
    "    # Calculate weighted sentiment scores\n",
    "    for sentiment in ['negative', 'neutral', 'positive', 'compound']:\n",
    "        chunk[f'weighted_{sentiment}'] = chunk.apply(lambda row: calculate_weighted_sentiment(row['score'], row[sentiment]), axis=1)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    chunk = chunk[['id', 'score', 'link_id', 'subreddit', 'created_utc', 'negative', 'neutral', 'positive', 'compound', 'weighted_negative', 'weighted_neutral', 'weighted_positive', 'weighted_compound']]\n",
    "    \n",
    "    processed_data.append(chunk)\n",
    "    chunk_count += 1\n",
    "    print(f\"Chunk {chunk_count} processed\")\n",
    "\n",
    "# Concatenate all processed chunks\n",
    "final_df = pd.concat(processed_data)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('processed_comments3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 301 processed\n",
      "Chunk 302 processed\n",
      "Chunk 303 processed\n",
      "Chunk 304 processed\n",
      "Chunk 305 processed\n",
      "Chunk 306 processed\n",
      "Chunk 307 processed\n",
      "Chunk 308 processed\n",
      "Chunk 309 processed\n",
      "Chunk 310 processed\n",
      "Chunk 311 processed\n",
      "Chunk 312 processed\n",
      "Chunk 313 processed\n",
      "Chunk 314 processed\n",
      "Chunk 315 processed\n",
      "Chunk 316 processed\n",
      "Chunk 317 processed\n",
      "Chunk 318 processed\n",
      "Chunk 319 processed\n",
      "Chunk 320 processed\n",
      "Chunk 321 processed\n",
      "Chunk 322 processed\n",
      "Chunk 323 processed\n",
      "Chunk 324 processed\n",
      "Chunk 325 processed\n",
      "Chunk 326 processed\n",
      "Chunk 327 processed\n",
      "Chunk 328 processed\n",
      "Chunk 329 processed\n",
      "Chunk 330 processed\n",
      "Chunk 331 processed\n",
      "Chunk 332 processed\n",
      "Chunk 333 processed\n",
      "Chunk 334 processed\n",
      "Chunk 335 processed\n",
      "Chunk 336 processed\n",
      "Chunk 337 processed\n",
      "Chunk 338 processed\n",
      "Chunk 339 processed\n",
      "Chunk 340 processed\n",
      "Chunk 341 processed\n",
      "Chunk 342 processed\n",
      "Chunk 343 processed\n",
      "Chunk 344 processed\n",
      "Chunk 345 processed\n",
      "Chunk 346 processed\n",
      "Chunk 347 processed\n",
      "Chunk 348 processed\n",
      "Chunk 349 processed\n",
      "Chunk 350 processed\n",
      "Chunk 351 processed\n",
      "Chunk 352 processed\n",
      "Chunk 353 processed\n",
      "Chunk 354 processed\n",
      "Chunk 355 processed\n",
      "Chunk 356 processed\n",
      "Chunk 357 processed\n",
      "Chunk 358 processed\n",
      "Chunk 359 processed\n",
      "Chunk 360 processed\n",
      "Chunk 361 processed\n",
      "Chunk 362 processed\n",
      "Chunk 363 processed\n",
      "Chunk 364 processed\n",
      "Chunk 365 processed\n",
      "Chunk 366 processed\n",
      "Chunk 367 processed\n",
      "Chunk 368 processed\n",
      "Chunk 369 processed\n",
      "Chunk 370 processed\n",
      "Chunk 371 processed\n",
      "Chunk 372 processed\n",
      "Chunk 373 processed\n",
      "Chunk 374 processed\n",
      "Chunk 375 processed\n",
      "Chunk 376 processed\n",
      "Chunk 377 processed\n",
      "Chunk 378 processed\n",
      "Chunk 379 processed\n",
      "Chunk 380 processed\n",
      "Chunk 381 processed\n",
      "Chunk 382 processed\n",
      "Chunk 383 processed\n",
      "Chunk 384 processed\n",
      "Chunk 385 processed\n",
      "Chunk 386 processed\n",
      "Chunk 387 processed\n",
      "Chunk 388 processed\n",
      "Chunk 389 processed\n",
      "Chunk 390 processed\n",
      "Chunk 391 processed\n",
      "Chunk 392 processed\n",
      "Chunk 393 processed\n",
      "Chunk 394 processed\n",
      "Chunk 395 processed\n",
      "Chunk 396 processed\n",
      "Chunk 397 processed\n",
      "Chunk 398 processed\n",
      "Chunk 399 processed\n",
      "Chunk 400 processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of rows to skip (10,000,000 already processed) and number of rows to read\n",
    "rows_to_skip = 30000000\n",
    "rows_to_read = 10000000\n",
    "\n",
    "# Read the CSV file in chunks, skipping the first 30,000,000 rows\n",
    "for chunk in pd.read_csv('text_comments.csv', chunksize=chunk_size, skiprows=range(1, rows_to_skip), nrows=rows_to_read, on_bad_lines='skip', lineterminator='\\n'):\n",
    "    # Drop rows where 'body' is null\n",
    "    chunk = chunk.dropna(subset=['body'])\n",
    "\n",
    "    # Convert 'created_utc' to standard datetime\n",
    "    chunk['created_utc'] = pd.to_datetime(chunk['created_utc'], unit='s')\n",
    "\n",
    "    # Calculate sentiment scores\n",
    "    chunk[['negative', 'neutral', 'positive', 'compound']] = chunk['body'].apply(lambda x: calculate_sentiment_scores(x)).apply(pd.Series)\n",
    "    \n",
    "    # Calculate weighted sentiment scores\n",
    "    for sentiment in ['negative', 'neutral', 'positive', 'compound']:\n",
    "        chunk[f'weighted_{sentiment}'] = chunk.apply(lambda row: calculate_weighted_sentiment(row['score'], row[sentiment]), axis=1)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    chunk = chunk[['id', 'score', 'link_id', 'subreddit', 'created_utc', 'negative', 'neutral', 'positive', 'compound', 'weighted_negative', 'weighted_neutral', 'weighted_positive', 'weighted_compound']]\n",
    "    \n",
    "    processed_data.append(chunk)\n",
    "    chunk_count += 1\n",
    "    print(f\"Chunk {chunk_count} processed\")\n",
    "\n",
    "# Concatenate all processed chunks\n",
    "final_df = pd.concat(processed_data)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('processed_comments3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 401 processed\n",
      "Chunk 402 processed\n",
      "Chunk 403 processed\n",
      "Chunk 404 processed\n",
      "Chunk 405 processed\n",
      "Chunk 406 processed\n",
      "Chunk 407 processed\n",
      "Chunk 408 processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of rows to skip (10,000,000 already processed) and number of rows to read\n",
    "rows_to_skip = 40000000\n",
    "rows_to_read = 10000000\n",
    "\n",
    "# Read the CSV file in chunks, skipping the first 40,000,000 rows\n",
    "for chunk in pd.read_csv('text_comments.csv', chunksize=chunk_size, skiprows=range(1, rows_to_skip), nrows=rows_to_read, on_bad_lines='skip', lineterminator='\\n'):\n",
    "    # Drop rows where 'body' is null\n",
    "    chunk = chunk.dropna(subset=['body'])\n",
    "\n",
    "    # Convert 'created_utc' to standard datetime\n",
    "    chunk['created_utc'] = pd.to_datetime(chunk['created_utc'], unit='s')\n",
    "\n",
    "    # Calculate sentiment scores\n",
    "    chunk[['negative', 'neutral', 'positive', 'compound']] = chunk['body'].apply(lambda x: calculate_sentiment_scores(x)).apply(pd.Series)\n",
    "    \n",
    "    # Calculate weighted sentiment scores\n",
    "    for sentiment in ['negative', 'neutral', 'positive', 'compound']:\n",
    "        chunk[f'weighted_{sentiment}'] = chunk.apply(lambda row: calculate_weighted_sentiment(row['score'], row[sentiment]), axis=1)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    chunk = chunk[['id', 'score', 'link_id', 'subreddit', 'created_utc', 'negative', 'neutral', 'positive', 'compound', 'weighted_negative', 'weighted_neutral', 'weighted_positive', 'weighted_compound']]\n",
    "    \n",
    "    processed_data.append(chunk)\n",
    "    chunk_count += 1\n",
    "    print(f\"Chunk {chunk_count} processed\")\n",
    "\n",
    "# Concatenate all processed chunks\n",
    "final_df = pd.concat(processed_data)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('processed_comments4.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cscd25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
